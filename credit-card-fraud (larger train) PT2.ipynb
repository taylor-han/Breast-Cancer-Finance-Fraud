{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import csv\n",
    "import random as rn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "import scipy.io as sio\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.mlab as mlab\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('creditcard.csv')\n",
    "df_ccf = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cases: 284807\n",
      "Number of fraud: 492\n",
      "Number of no fraud: 284315\n"
     ]
    }
   ],
   "source": [
    "number_fraud = len(data[data.Class == 1])\n",
    "number_no_fraud = len(data[data.Class == 0])\n",
    "print('Total cases: {}'.format(number_no_fraud + number_fraud))\n",
    "print('Number of fraud: {}'.format(number_fraud))\n",
    "print('Number of no fraud: {}'.format(number_no_fraud))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frauds in this training dataset: 492\n"
     ]
    }
   ],
   "source": [
    "df_train_1 = df_ccf[df_ccf['Class'] == 1]\n",
    "df_train_0 = df_ccf[df_ccf['Class'] == 0]\n",
    "len_df_train_1 = len(df_train_1)\n",
    "print('Number of frauds in this training dataset: {}'.format(len_df_train_1))\n",
    "# Append similar number of non-fraud samples into this training dataset\n",
    "df_sample = df_train_0.sample(500)\n",
    "df_train = df_train_1.append(df_sample)\n",
    "# Mix our dataset\n",
    "df_train = df_train.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Time and the Class (label)\n",
    "X = df_train.drop(['Time', 'Class'],axis=1)\n",
    "Y = df_train['Class'] # We create our label\n",
    "X = np.asarray(X)\n",
    "Y = np.asarray(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition 2 (80/20 Split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val, X_test, Y_train_val, Y_test = train_test_split(X, Y, test_size = 0.2 ,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate error given feature vectors X and labels Y.\n",
    "def calc_error(X, Y, classifier):\n",
    "    Y_pred = classifier.predict(X)   \n",
    "    e = 1 - accuracy_score(Y, Y_pred) \n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 0.1\n",
      "Decision boundary: -0.120x0+-0.194x1+-1.169=0\n",
      "Training accuracy: 0.9482976040353089\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "C = 1\n",
      "Decision boundary: -0.209x0+-0.348x1+-1.356=0\n",
      "Training accuracy: 0.9129886506935687\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "C = 10\n",
      "Decision boundary: 0.155x0+0.084x1+-1.910=0\n",
      "Training accuracy: 0.9344262295081968\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "C = 100\n",
      "Decision boundary: 2.002x0+3.968x1+-6.014=0\n",
      "Training accuracy: 0.9003783102143758\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "C = 1000\n",
      "Decision boundary: 3.159x0+8.215x1+-8.780=0\n",
      "Training accuracy: 0.78562421185372\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "C_list = [0.1, 1, 10, 100, 1000]\n",
    "opt_e_training = 1.0   # Optimal training error.\n",
    "opt_classifier = None  # Optimal classifier.\n",
    "opt_C          = None  # Optimal C.\n",
    "\n",
    "for C in C_list:\n",
    "    # Create a linear SVM classifier.\n",
    "    classifier = svm.LinearSVC(penalty='l2', loss='hinge', C=C, max_iter=10000)\n",
    "    \n",
    "    # Use the classifier to fit the training set (use X_train, Y_train).\n",
    "    classifier.fit(X_train_val, Y_train_val)\n",
    "\n",
    "    # Obtain the weights and bias from the linear SVM classifier.\n",
    "    W = classifier.coef_[0]\n",
    "    b = classifier.intercept_[0]\n",
    "    \n",
    "    # Show decision boundary, training error and test error.\n",
    "    print('C = {}'.format(C))\n",
    "    print('Decision boundary: {:.3f}x0+{:.3f}x1+{:.3f}=0'.format(W[0],W[1],b))\n",
    "    #vis(X_train_val, Y_train_val, W, b)\n",
    "    e_training = calc_error(X_train_val, Y_train_val, classifier)\n",
    "    print('Training accuracy: {}'.format(1 - e_training))\n",
    "    print('\\n\\n\\n')\n",
    "    \n",
    "    # Judge if it is the optimal one.\n",
    "    if e_training < opt_e_training:\n",
    "        opt_e_training = e_training\n",
    "        opt_classifier = classifier\n",
    "        opt_C = C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter C* = 0.1\n",
      "Decision boundary: -0.120x0+-0.194x1+-1.169=0\n",
      "Testing Accuracy: 0.9296482412060302\n"
     ]
    }
   ],
   "source": [
    "# Obtain the weights and bias from the best linear SVM classifier.\n",
    "opt_W = opt_classifier.coef_[0]\n",
    "opt_b = opt_classifier.intercept_[0]\n",
    "print('Best parameter C* = {}'.format(opt_C))\n",
    "print('Decision boundary: {:.3f}x0+{:.3f}x1+{:.3f}=0'.format(opt_W[0],opt_W[1],opt_b))\n",
    "#vis(X_test, Y_test, opt_W, opt_b)\n",
    "print('Testing Accuracy: {}'.format(1 - calc_error(X_test, Y_test, opt_classifier)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter: {'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Training accuracy: [0.94640602 0.95144837]\n",
      "Validation score: [0.9407314 0.9407314]\n",
      "Testining accuracy: 0.9547738693467337\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegressionCV()\n",
    "param = [\n",
    "  {'penalty': ['l1'], 'solver': ['liblinear']},\n",
    "  {'penalty': ['l2'], 'solver': ['newton-cg']},\n",
    " ]\n",
    "logit = GridSearchCV(classifier, param, cv=5, return_train_score=True)\n",
    "logit.fit(X_train_val, Y_train_val)\n",
    "logit_train = logit.cv_results_['mean_train_score']\n",
    "logit_test = logit.cv_results_['mean_test_score']\n",
    "\n",
    "test_accX = logit.best_estimator_.predict(X_test) == Y_test\n",
    "print('Best parameter: {}'.format(logit.best_params_))\n",
    "#print(sum(test_accX))\n",
    "test_acc = sum(test_accX)/len(test_accX)\n",
    "print('Training accuracy: {}'.format(logit_train))\n",
    "print('Validation score: {}'.format(logit_test))\n",
    "print('Testining accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter: {'max_features': 0.1, 'n_estimators': 1024}\n",
      "Training accuracy: [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Validation score: [0.92812106 0.93568726 0.93442623 0.93568726 0.9331652  0.9331652\n",
      " 0.92812106 0.92812106]\n",
      "Testining accuracy: 0.9597989949748744\n"
     ]
    }
   ],
   "source": [
    "param = {\"max_features\":[0.05,0.1,0.2,0.3,0.4,0.6,0.8,1],\"n_estimators\":[1024]}\n",
    "classifier = RandomForestClassifier()\n",
    "\n",
    "rf = GridSearchCV(classifier,param,cv=5, return_train_score=True)\n",
    "rf.fit(X_train_val, Y_train_val)\n",
    "rf_train = rf.cv_results_['mean_train_score']\n",
    "rf_test = rf.cv_results_['mean_test_score']\n",
    "\n",
    "test_accX = rf.best_estimator_.predict(X_test) == Y_test\n",
    "\n",
    "#print(sum(test_accX))\n",
    "test_acc = sum(test_accX)/len(test_accX)\n",
    "print('Best parameter: {}'.format(rf.best_params_))\n",
    "print('Training accuracy: {}'.format(rf_train))\n",
    "print('Validation score: {}'.format(rf_test))\n",
    "print('Testining accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter: {'criterion': 'entropy', 'max_depth': 5}\n",
      "Training accuracy: [0.91078219 0.91456767 0.93473956 0.94766984 0.96469013]\n",
      "Validation score: [0.90920555 0.90290038 0.90163934 0.90416141 0.91424968]\n",
      "Testining accuracy: 0.9095477386934674\n"
     ]
    }
   ],
   "source": [
    "param  = {\"criterion\":[\"entropy\"],'max_depth':[1,2,3,4,5]}\n",
    "classifier = DecisionTreeClassifier()\n",
    "\n",
    "dt = GridSearchCV(classifier,param,cv=5, return_train_score=True)\n",
    "dt.fit(X_train_val, Y_train_val)\n",
    "dt_train = dt.cv_results_['mean_train_score']\n",
    "dt_test = dt.cv_results_['mean_test_score']\n",
    "\n",
    "test_accX = dt.best_estimator_.predict(X_test) == Y_test\n",
    "#print(sum(test_accX))\n",
    "test_acc = sum(test_accX)/len(test_accX)\n",
    "print('Best parameter: {}'.format(dt.best_params_))\n",
    "print('Training accuracy: {}'.format(dt_train))\n",
    "print('Validation score: {}'.format(dt_test))\n",
    "print('Testining accuracy: {}'.format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
